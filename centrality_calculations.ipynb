{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "conda deactivate\n",
    "conda env remove --name networks\n",
    "\n",
    "mamba create --prefix /DeepenData/.miniconda/envs/networks \\\n",
    "      scipy ray-default grpcio networkx pandas -c conda-forge -c anaconda --yes\n",
    "\n",
    "\n",
    "conda activate networks\n",
    "\n",
    "mamba install -C -S botocore boto3 --yes\n",
    "\n",
    "pip install --upgrade setuptools\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "mamba install -c conda-forge apache-airflow \n",
    "\n",
    "conda env export --name networks > networks.yml\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  #\n",
    "import logging\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import ray\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_largest_connected_component(graph: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Find and return the largest connected component of a graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: The largest connected subgraph.\n",
    "    \"\"\"\n",
    "    assert isinstance(graph, nx.Graph), \"Input must be a networkx Graph.\"\n",
    "    \n",
    "    try:\n",
    "        largest_component = max(nx.connected_components(graph), key=len)\n",
    "        largest_connected_subgraph = graph.subgraph(largest_component).copy()\n",
    "        assert nx.is_connected(largest_connected_subgraph), \"Output subgraph is not connected.\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to compute largest connected component: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return largest_connected_subgraph\n",
    "\n",
    "\n",
    "\n",
    "def calculate_quick_centralities(graph: nx.Graph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute some of the faster-to-calculate centralities and return them in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the calculated centralities.\n",
    "    \"\"\"\n",
    "    assert isinstance(graph, nx.Graph), \"Input must be a networkx Graph.\"\n",
    "\n",
    "    try:\n",
    "        # Degree centrality\n",
    "        degree_centrality = nx.degree_centrality(graph)\n",
    "\n",
    "        # Eigenvector centrality\n",
    "        eigenvector_centrality = nx.eigenvector_centrality(graph, max_iter=1000, tol=1e-05)\n",
    "\n",
    "        # Closeness centrality\n",
    "        closeness_centrality = nx.closeness_centrality(graph)\n",
    "\n",
    "        # Information centrality\n",
    "        info_centrality = nx.information_centrality(graph)\n",
    "\n",
    "        centralities = {\n",
    "            \"degree_centrality\": degree_centrality,\n",
    "            \"eigenvector_centrality\": eigenvector_centrality,\n",
    "            \"closeness_centrality\": closeness_centrality,\n",
    "            \"information_centrality\": info_centrality,\n",
    "        }\n",
    "\n",
    "        df_centralities = pd.DataFrame(centralities)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to compute quick centralities: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return df_centralities\n",
    "\n",
    "def calculate_centralities(graph: nx.Graph, use_quick_measurements: bool=False, alpha: float=0.005) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute several centralities and return them in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        use_quick_measurements (bool): Flag to use quick measurements. Default is False.\n",
    "        alpha (float): The damping factor for Katz centrality and PageRank. Default is 0.005.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the calculated centralities.\n",
    "    \"\"\"\n",
    "    assert isinstance(graph, nx.Graph), \"Input must be a networkx Graph.\"\n",
    "    assert isinstance(use_quick_measurements, bool), \"use_quick_measurements must be a boolean.\"\n",
    "    assert isinstance(alpha, (int, float)), \"alpha must be a numeric type.\"\n",
    "\n",
    "    try:\n",
    "        if not use_quick_measurements:\n",
    "            centralities = {\n",
    "                \"degree_centrality\": nx.degree_centrality(graph),\n",
    "                \"harmonic_centrality\": nx.harmonic_centrality(graph),\n",
    "                \"eigenvector_centrality\": nx.eigenvector_centrality(graph, max_iter=1000, tol=1e-05),\n",
    "                \"betweenness_centrality\": nx.betweenness_centrality(graph, normalized=True),\n",
    "                \"closeness_centrality\": nx.closeness_centrality(graph, wf_improved=True),\n",
    "                \"load_centrality\": nx.load_centrality(graph, normalized=True),\n",
    "                \"information_centrality\": nx.information_centrality(graph),\n",
    "                \"katz_centrality\": nx.katz_centrality(graph, alpha=alpha),\n",
    "                \"pagerank\": nx.pagerank(graph, alpha=alpha),\n",
    "            }\n",
    "        else:\n",
    "            centralities = calculate_quick_centralities(graph)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to compute centralities: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return pd.DataFrame(centralities)\n",
    "\n",
    "\n",
    "def compute_alpha_for_graph(graph: nx.Graph) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the alpha parameter for Katz centrality and PageRank based on the graph's largest eigenvalue.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated alpha.\n",
    "    \"\"\"\n",
    "    assert isinstance(graph, nx.Graph), \"Input must be a networkx Graph.\"\n",
    "\n",
    "    try:\n",
    "        adjacency_matrix = nx.adjacency_matrix(graph).toarray()\n",
    "        largest_eigenvalue = np.max(np.linalg.eigvals(adjacency_matrix))\n",
    "        alpha = 0.9 * (1 / np.real(largest_eigenvalue))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to compute alpha: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def remove_node_and_calculate_centralities(graph: nx.Graph, node_to_remove, verbose: bool=False):\n",
    "    \"\"\"\n",
    "    Remove a node from the graph, then calculate and return the centralities of the remaining nodes.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        node_to_remove: The node to be removed.\n",
    "        verbose (bool): Flag to print verbose messages. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the removed node and a DataFrame of the new centralities.\n",
    "    \"\"\"\n",
    "    assert isinstance(graph, nx.Graph), \"Input must be a networkx Graph.\"\n",
    "    assert isinstance(verbose, bool), \"verbose must be a boolean.\"\n",
    "\n",
    "    try:\n",
    "        graph_copy = graph.copy()\n",
    "        graph_copy.remove_node(node_to_remove)\n",
    "        largest_connected_subgraph = get_largest_connected_component(graph_copy)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to compute largest connected component: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    assert len(graph.nodes) != len(largest_connected_subgraph.nodes), \"No node was removed.\"\n",
    "\n",
    "    removed_nodes = set(graph.nodes) - set(largest_connected_subgraph.nodes)\n",
    "\n",
    "    if verbose:\n",
    "        logging.info(f\"Nodes removed: {removed_nodes}\")\n",
    "\n",
    "    try:\n",
    "        new_centralities = calculate_centralities(largest_connected_subgraph, use_quick_measurements=False, alpha=compute_alpha_for_graph(largest_connected_subgraph))\n",
    "        new_centralities = new_centralities.reindex(list(graph.nodes))\n",
    "        new_centralities.name = str(node_to_remove)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to calculate centralities: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Check that removed nodes have NaN centralities\n",
    "    for removed_node in removed_nodes:\n",
    "        assert np.isnan(new_centralities.loc[removed_node, \"eigenvector_centrality\"]), \"Removed node has non-NaN value.\"\n",
    "\n",
    "    return node_to_remove, new_centralities\n",
    "\n",
    "\n",
    "\n",
    "def remove_nodes_and_calculate_centralities(graph: nx.Graph, nodes_to_remove: list):\n",
    "    \"\"\"\n",
    "    Remove a list of nodes from the graph and calculate the centralities after each removal.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        nodes_to_remove (list): List of nodes to be removed.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with removed node and new centralities.\n",
    "    \"\"\"\n",
    "    # Checking input types\n",
    "    assert isinstance(graph, nx.Graph), \"Input must be a networkx Graph.\"\n",
    "    assert isinstance(nodes_to_remove, list), \"nodes_to_remove must be a list.\"\n",
    "\n",
    "    # Check if all nodes to remove are in the graph, and create a list of nodes in the graph\n",
    "    nodes_in_graph = [node for node in nodes_to_remove if node in graph.nodes()]\n",
    "\n",
    "    # If any nodes_to_remove were not found in the graph, log a warning\n",
    "    if len(nodes_to_remove) != len(nodes_in_graph):\n",
    "        logging.warning(f\"Not all nodes are in the graph ({len(nodes_in_graph)}/{len(nodes_to_remove)}).\")\n",
    "\n",
    "    try:\n",
    "        # Here, we use Ray to execute the remove_node_and_calculate_centralities function in parallel for each node\n",
    "        # The .remote() function call tells Ray to execute the function as a remote task\n",
    "        # This line does not actually execute the tasks yet, but creates futures for them\n",
    "        futures = [remove_node_and_calculate_centralities.remote(graph, node) for node in nodes_in_graph]\n",
    "\n",
    "        # This line triggers the execution of the remote tasks and blocks until all tasks have completed\n",
    "        # The results of the tasks are returned as a list in the same order as the futures\n",
    "        result = ray.get(futures)\n",
    "    except Exception as e:\n",
    "        # If anything goes wrong during the execution of the tasks, log an error and re-raise the exception\n",
    "        logging.error(f\"Failed to remove nodes and calculate centralities: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Return the result, which is a list of (removed_node, new_centralities) tuples\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:46:26,297\tINFO worker.py:1616 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "#import networkx as nx\n",
    " \n",
    "graphml_path: str = \"graph.graphml\"\n",
    "Gnx = nx.read_graphml(graphml_path)  #\n",
    "Gnx = nx.Graph(Gnx)\n",
    "\n",
    "G = Gnx.copy()\n",
    "\n",
    "to_remove = list(Gnx.nodes)[60:70]\n",
    "\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "\n",
    "centralidades_perturbadas = remove_nodes_and_calculate_centralities(G, to_remove)\n",
    "\n",
    "ray.shutdown()\n",
    "baseline = calculate_centralities(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "total nodes: 1056\n",
      "total centralites: 9\n",
      "nodes removed: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f'''\n",
    "total nodes: {centralidades_perturbadas[0][1].shape[0]}\n",
    "total centralites: {centralidades_perturbadas[0][1].shape[1]}\n",
    "nodes removed: {len(centralidades_perturbadas)}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'centralidades_perturbadas' : centralidades_perturbadas,\n",
    "#  'baseline': baseline}\n",
    "import pickle\n",
    " \n",
    " \n",
    "with open('centralities.pickle', 'wb') as handle:\n",
    "    pickle.dump({\n",
    "        'centralidades_perturbadas' : centralidades_perturbadas,\n",
    "        'baseline' : baseline\n",
    "    }, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a list to store all the DataFrames (baseline and perturbed centralities)\n",
    "all_centralities = []\n",
    "index_keys = []\n",
    "\n",
    "# Add baseline centralities\n",
    "all_centralities.append(baseline)\n",
    "index_keys.append(\"baseline\")\n",
    "\n",
    "# Iterate over the perturbed centralities\n",
    "for node in centralidades_perturbadas:\n",
    "    # Get the centrality values and the name of the removed node\n",
    "    perturbed_centralities = node[1]\n",
    "    removed_node_name = node[0]\n",
    "\n",
    "    # Add them to the list\n",
    "    all_centralities.append(perturbed_centralities)\n",
    "    index_keys.append(removed_node_name)\n",
    "\n",
    "# Concatenate all DataFrames into one, with a multi-index\n",
    "centralidades_df = pd.concat(\n",
    "    all_centralities,  # DataFrames to concatenate\n",
    "    axis=0,  # Concatenate row-wise\n",
    "    keys=index_keys,  # Keys for the multi-index\n",
    "    names=[\"removed_node\", \"metabolite\"],  # Names for the levels of the multi-index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>harmonic_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>load_centrality</th>\n",
       "      <th>information_centrality</th>\n",
       "      <th>katz_centrality</th>\n",
       "      <th>pagerank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>removed_node</th>\n",
       "      <th>metabolite</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10FTHF5GLUtm</th>\n",
       "      <th>10FTHF5GLUtm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10FTHF5GLUtm_Neuron</th>\n",
       "      <td>-0.002738</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>-2.345901e-04</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.005308</td>\n",
       "      <td>1.167213</td>\n",
       "      <td>-0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10FTHF6GLUtm</th>\n",
       "      <td>-0.002738</td>\n",
       "      <td>0.004088</td>\n",
       "      <td>6.412848e-05</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.005273</td>\n",
       "      <td>1.140553</td>\n",
       "      <td>-0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10FTHF6GLUtm_Neuron</th>\n",
       "      <td>-0.002738</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>-2.345900e-04</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.001883</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.005308</td>\n",
       "      <td>1.167216</td>\n",
       "      <td>-0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10FTHF7GLUtm</th>\n",
       "      <td>-0.002738</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>-2.193122e-06</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.004293</td>\n",
       "      <td>1.205493</td>\n",
       "      <td>-0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">sink_tyr-L(m)</th>\n",
       "      <th>sink_tyr-L(m)_Neuron</th>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>-5.001942e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002160</td>\n",
       "      <td>1.267533</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sink_vitd2</th>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>-1.900514e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001819</td>\n",
       "      <td>1.276183</td>\n",
       "      <td>-0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sink_vitd2_Neuron</th>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>-5.028246e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>1.276278</td>\n",
       "      <td>-0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sink_vitd3</th>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>-1.907357e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002170</td>\n",
       "      <td>1.274042</td>\n",
       "      <td>-0.000824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sink_vitd3_Neuron</th>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-5.028596e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002176</td>\n",
       "      <td>1.274234</td>\n",
       "      <td>-0.000824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11616 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    degree_centrality  harmonic_centrality  \\\n",
       "removed_node  metabolite                                                     \n",
       "10FTHF5GLUtm  10FTHF5GLUtm                        NaN                  NaN   \n",
       "              10FTHF5GLUtm_Neuron           -0.002738             0.001351   \n",
       "              10FTHF6GLUtm                  -0.002738             0.004088   \n",
       "              10FTHF6GLUtm_Neuron           -0.002738             0.001351   \n",
       "              10FTHF7GLUtm                  -0.002738             0.002868   \n",
       "...                                               ...                  ...   \n",
       "sink_tyr-L(m) sink_tyr-L(m)_Neuron          -0.001368             0.000819   \n",
       "              sink_vitd2                    -0.001368             0.001301   \n",
       "              sink_vitd2_Neuron             -0.001368             0.000834   \n",
       "              sink_vitd3                    -0.001368             0.001297   \n",
       "              sink_vitd3_Neuron             -0.001368             0.000831   \n",
       "\n",
       "                                    eigenvector_centrality  \\\n",
       "removed_node  metabolite                                     \n",
       "10FTHF5GLUtm  10FTHF5GLUtm                             NaN   \n",
       "              10FTHF5GLUtm_Neuron            -2.345901e-04   \n",
       "              10FTHF6GLUtm                    6.412848e-05   \n",
       "              10FTHF6GLUtm_Neuron            -2.345900e-04   \n",
       "              10FTHF7GLUtm                   -2.193122e-06   \n",
       "...                                                    ...   \n",
       "sink_tyr-L(m) sink_tyr-L(m)_Neuron           -5.001942e-06   \n",
       "              sink_vitd2                     -1.900514e-08   \n",
       "              sink_vitd2_Neuron              -5.028246e-06   \n",
       "              sink_vitd3                     -1.907357e-08   \n",
       "              sink_vitd3_Neuron              -5.028596e-06   \n",
       "\n",
       "                                    betweenness_centrality  \\\n",
       "removed_node  metabolite                                     \n",
       "10FTHF5GLUtm  10FTHF5GLUtm                             NaN   \n",
       "              10FTHF5GLUtm_Neuron                -0.002738   \n",
       "              10FTHF6GLUtm                       -0.002738   \n",
       "              10FTHF6GLUtm_Neuron                -0.002738   \n",
       "              10FTHF7GLUtm                       -0.002738   \n",
       "...                                                    ...   \n",
       "sink_tyr-L(m) sink_tyr-L(m)_Neuron                     NaN   \n",
       "              sink_vitd2                               NaN   \n",
       "              sink_vitd2_Neuron                        NaN   \n",
       "              sink_vitd3                               NaN   \n",
       "              sink_vitd3_Neuron                        NaN   \n",
       "\n",
       "                                    closeness_centrality  load_centrality  \\\n",
       "removed_node  metabolite                                                    \n",
       "10FTHF5GLUtm  10FTHF5GLUtm                           NaN              NaN   \n",
       "              10FTHF5GLUtm_Neuron              -0.001882        -0.002738   \n",
       "              10FTHF6GLUtm                      0.001141        -0.002738   \n",
       "              10FTHF6GLUtm_Neuron              -0.001883        -0.002738   \n",
       "              10FTHF7GLUtm                      0.000504        -0.002738   \n",
       "...                                                  ...              ...   \n",
       "sink_tyr-L(m) sink_tyr-L(m)_Neuron             -0.000655              NaN   \n",
       "              sink_vitd2                        0.000076              NaN   \n",
       "              sink_vitd2_Neuron                -0.000638              NaN   \n",
       "              sink_vitd3                        0.000075              NaN   \n",
       "              sink_vitd3_Neuron                -0.000639              NaN   \n",
       "\n",
       "                                    information_centrality  katz_centrality  \\\n",
       "removed_node  metabolite                                                      \n",
       "10FTHF5GLUtm  10FTHF5GLUtm                             NaN              NaN   \n",
       "              10FTHF5GLUtm_Neuron                -0.005308         1.167213   \n",
       "              10FTHF6GLUtm                       -0.005273         1.140553   \n",
       "              10FTHF6GLUtm_Neuron                -0.005308         1.167216   \n",
       "              10FTHF7GLUtm                       -0.004293         1.205493   \n",
       "...                                                    ...              ...   \n",
       "sink_tyr-L(m) sink_tyr-L(m)_Neuron               -0.002160         1.267533   \n",
       "              sink_vitd2                         -0.001819         1.276183   \n",
       "              sink_vitd2_Neuron                  -0.001823         1.276278   \n",
       "              sink_vitd3                         -0.002170         1.274042   \n",
       "              sink_vitd3_Neuron                  -0.002176         1.274234   \n",
       "\n",
       "                                    pagerank  \n",
       "removed_node  metabolite                      \n",
       "10FTHF5GLUtm  10FTHF5GLUtm               NaN  \n",
       "              10FTHF5GLUtm_Neuron  -0.002741  \n",
       "              10FTHF6GLUtm         -0.002741  \n",
       "              10FTHF6GLUtm_Neuron  -0.002741  \n",
       "              10FTHF7GLUtm         -0.002735  \n",
       "...                                      ...  \n",
       "sink_tyr-L(m) sink_tyr-L(m)_Neuron  0.000221  \n",
       "              sink_vitd2           -0.000555  \n",
       "              sink_vitd2_Neuron    -0.000555  \n",
       "              sink_vitd3           -0.000824  \n",
       "              sink_vitd3_Neuron    -0.000824  \n",
       "\n",
       "[11616 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class CentralityAnalyser:\n",
    "    \"\"\"Analyse centrality values for different removed nodes in a network.\"\"\"\n",
    "\n",
    "    def __init__(self, centralities_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the centrality analyser with a dataframe of centrality values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        centralities_df : pd.DataFrame\n",
    "            A dataframe containing the centrality values for different removed nodes.\n",
    "        \"\"\"\n",
    "        self.centralities_df: pd.DataFrame = centralities_df.sort_index()\n",
    "        self.baseline: pd.DataFrame = self.centralities_df.loc[\"baseline\"]\n",
    "    \n",
    "    def calculate_log2_ratio_centrality(self, removed_node: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate the log2 ratio of centrality values relative to baseline for a given removed node.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        removed_node : str\n",
    "            The node that was removed.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A dataframe containing the log2 ratio of centrality values for the removed node.\n",
    "        \"\"\"\n",
    "        assert all(self.baseline.index.values == self.centralities_df.loc[removed_node].index.values), \"Index mismatch\"\n",
    "\n",
    "        centrality_ratio = np.log2(self.baseline / self.centralities_df.loc[removed_node])\n",
    "        centrality_ratio[\"removed_node\"] = removed_node\n",
    "\n",
    "        return centrality_ratio.reset_index().set_index([\"removed_node\", \"metabolite\"])\n",
    "\n",
    "    def calculate_centrality_variation(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate the centrality variation for all removed nodes.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A dataframe containing the centrality variation for all removed nodes.\n",
    "        \"\"\"\n",
    "        all_removed_nodes: List[str] = self.centralities_df.index.get_level_values(\"removed_node\").unique()\n",
    "\n",
    "        centrality_variations = [self.calculate_log2_ratio_centrality(node) for node in all_removed_nodes]\n",
    "\n",
    "        return pd.concat(centrality_variations)\n",
    "\n",
    "# \n",
    "analyser = CentralityAnalyser(centralidades_df)\n",
    "log2_ratio_df = analyser.calculate_centrality_variation()\n",
    "log2_ratio_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
